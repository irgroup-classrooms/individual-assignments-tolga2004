{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIS08 / OR92 Data Modeling: Python - Web Scraping\n",
    "\n",
    "Timo Breuer, Faculty of Information Science and Communication Studies, Institute of Information Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** Some contents are taken from https://carpentries-incubator.github.io/lc-webscraping/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Basics of the Web, HTTP, and HTML\n",
    "---\n",
    "\n",
    "### What is the Web?\n",
    "\n",
    "- A collection of interconnected documents and resources accessed via the internet.\n",
    "- Web browsers (e.g., Chrome, Firefox, Safari) are used to navigate the web.\n",
    "\n",
    "### What is HTTP?\n",
    "\n",
    "**HTTP = HyperText Transfer Protocol:**\n",
    "- It is a protocol for transferring data between a client (browser) and a server.\n",
    "  \n",
    "**Key Features:**\n",
    "- Request-Response Model\n",
    "- Stateless (no memory of previous requests).\n",
    "  \n",
    "**HTTP Methods:**\n",
    "- `GET`: Retrieve data.\n",
    "- `POST`: Send data.\n",
    "- `PUT`: Update data.\n",
    "- `DELETE`: Remove data.\n",
    "\n",
    "Example Request:\n",
    "```\n",
    "GET /index.html HTTP/1.1\n",
    "Host: example.com\n",
    "```\n",
    "\n",
    "Example Response:\n",
    "```\n",
    "HTTP/1.1 200 OK\n",
    "Content-Type: text/html\n",
    "```\n",
    "\n",
    "### What is HTML?\n",
    "\n",
    "**HTML (HyperText Markup Language):**\n",
    "- Language for structuring web pages.\n",
    "- Defines content and layout.\n",
    "\n",
    "**HTML Elements:**\n",
    "- Exmaple tags: `<h1>`, `<p>`, `<a>`.\n",
    "- Example attributes: class, id, href.\n",
    "\n",
    "**Basic HTML Example:**\n",
    "```\n",
    "<html>\n",
    "  <head>\n",
    "    <title>My Web Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Welcome to My Page</h1>\n",
    "    <p>This is a paragraph.</p>\n",
    "    <a href=\"https://example.com\">Click Here</a>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "\n",
    "### How Does a Web Page Load?\n",
    "\n",
    "**Step-by-Step Process:**\n",
    "1.\tBrowser sends an HTTP request to a server.\n",
    "2.\tServer processes the request and sends an HTTP response (usually HTML).\n",
    "3.\tBrowser renders the HTML, CSS, and JavaScript to display the page.\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNp1UctuwjAQ_BXLJ5DSNDHkeeBQ-qBVKyGSU5WLG2-TCGKntlOgiH-vk0APVFhaybs7M7v2HHAuGOAYK_hqgedwX9FC0jrjyJyGSl3lVUO5RndSbBXI_40E5HdXHzon2M1sNtRj0-cMLdJ0iVbdDKXRCOzCttDTQ4puK85gZ5e63owHgYFm-Cel-MxVjeAK0GiRvr2iueAauB5fDv0jLak04A57ba9H0HmJNhVfA0MSlGhlDgqN5klioRcTzzUtQF1dq3_XJf3qPiuDBom28NEYVWzhGmRNK2a-_tCRMqxLqCHDsbkyKtcZzvjR4GirRbLnOY61bMHCUrRFeU7ahlF99uxcNK68C2HST7pRJgdWaSHfBp97u3sMjg94h-NgYpPImYau5weTkISuhfc4Jq4duST0AsfzfH_q-ORo4Z9e1bUdhxA_ckjku8HEifzjL6b9u78?type=png)](https://mermaid.live/edit#pako:eNp1UctuwjAQ_BXLJ5DSNDHkeeBQ-qBVKyGSU5WLG2-TCGKntlOgiH-vk0APVFhaybs7M7v2HHAuGOAYK_hqgedwX9FC0jrjyJyGSl3lVUO5RndSbBXI_40E5HdXHzon2M1sNtRj0-cMLdJ0iVbdDKXRCOzCttDTQ4puK85gZ5e63owHgYFm-Cel-MxVjeAK0GiRvr2iueAauB5fDv0jLak04A57ba9H0HmJNhVfA0MSlGhlDgqN5klioRcTzzUtQF1dq3_XJf3qPiuDBom28NEYVWzhGmRNK2a-_tCRMqxLqCHDsbkyKtcZzvjR4GirRbLnOY61bMHCUrRFeU7ahlF99uxcNK68C2HST7pRJgdWaSHfBp97u3sMjg94h-NgYpPImYau5weTkISuhfc4Jq4duST0AsfzfH_q-ORo4Z9e1bUdhxA_ckjku8HEifzjL6b9u78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What is Web Scraping?\n",
    "---\n",
    "\n",
    "**Definition:** Extracting data from websites programmatically.\n",
    "\n",
    "**Why scrape?**\n",
    "- Converts non-tabular or poorly structured data into usable formats like .csv files, spreadsheets, or database-compatible formats.\n",
    "- Access information not available via APIs.\n",
    "- Automate data collection for analysis.\n",
    "\n",
    "**Use Cases:**\n",
    "- Data Collection: Acquire specific data points from websites.\n",
    "- Data Archiving: Preserve online data for future reference.\n",
    "- Change Tracking: Monitor updates or changes to online information\n",
    "- Examples:\n",
    "  - Data for machine learning.\n",
    "  - Price comparison/Competitive Analysis: Businesses scrape competitor prices to adjust their own.\n",
    "  - Contact Scraping: Collecting personal information for marketing purposes.\n",
    "  - Academic Research/Trend Analysis: Creating datasets for text mining and analysis in scholarly projects.\n",
    "  - Data Journalism: Investigative journalists scrape data for stories, especially when not easily accessible.\n",
    "\n",
    "---\n",
    "## Tools for Web Scraping\n",
    "---\n",
    "\n",
    "**Languages:** Python (preferred), but also PHP or Ruby (i.e., other scripting languages).\n",
    "\n",
    "**Python Libraries:**\n",
    "- **Requests:** https://requests.readthedocs.io/en/latest/  \n",
    "- **Beautiful Soup:** https://beautiful-soup-4.readthedocs.io/\n",
    "- **Scrapy:** https://scrapy.org/\n",
    "- **Selenium:** https://selenium-python.readthedocs.io/ \n",
    "- **Parsel:** https://parsel.readthedocs.io/en/latest/\n",
    "\n",
    "**Other Tools:**\n",
    "- **Browser Developer Tools** for inspection (Shortcut: `F12`)\n",
    "- **Regex** for string patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A first example\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a local copy of the `index.html`. (Later you can keep it in memory without writing it to the disk.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Specify the URL of the page you want to download\n",
    "url = \"https://www.scrapethissite.com/pages/simple/\"\n",
    "\n",
    "# Make a GET request to fetch the raw HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the content to an HTML file\n",
    "    with open(\"index.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(response.text)\n",
    "    print(\"Page saved as 'page.html'\")\n",
    "else:\n",
    "    print(f\"Failed to download page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the file and get familiar with the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As an alternative you can also use the Developer Tools of your web browser!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# If the HTML is saved locally, you can use:\n",
    "with open(\"index.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "# Initialize an empty dictionary to store the country data\n",
    "countries_dict = {}\n",
    "\n",
    "# Find all div elements with class 'col-md-4 country'\n",
    "countries = soup.find_all('div', class_='col-md-4 country')\n",
    "\n",
    "# Iterate over each country div\n",
    "for country in countries:\n",
    "    # Extract country name\n",
    "    name = country.find('h3', class_='country-name').get_text(strip=True).split('\\n')[-1].strip()\n",
    "    \n",
    "    # Extract capital, population, and area\n",
    "    capital = country.find('span', class_='country-capital').get_text(strip=True)\n",
    "    population = country.find('span', class_='country-population').get_text(strip=True)\n",
    "    area = country.find('span', class_='country-area').get_text(strip=True)\n",
    "\n",
    "    # Store the data in the dictionary\n",
    "    countries_dict[name] = {\n",
    "        'Capital': capital,\n",
    "        'Population': population,\n",
    "        'Area (km^2)': area\n",
    "    }\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for country, data in countries_dict.items():\n",
    "    print(f\"{country}: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "## Don’t break the web: Denial of Service attacks\n",
    "---\n",
    "\n",
    "**Web Scraping Basics:**\n",
    "- Web scraping involves repeatedly querying a website and accessing multiple pages.\n",
    "- Each request to a web server consumes its resources, potentially impacting other users.\n",
    "\n",
    "**Risks of Excessive Requests:**\n",
    "- Sending too many requests in a short time can overload a server.\n",
    "- Overloading may block legitimate users or crash the server.\n",
    "- Hackers exploit this technique intentionally in Denial of Service (DoS) attacks.\n",
    "\n",
    "**Server Protections Against Abuse:**\n",
    "- Servers monitor for excessive requests from a single IP address.\n",
    "- They may block or ban the source to prevent misuse.\n",
    "\n",
    "**Challenges for Scrapers:**\n",
    "- Legitimate web scrapers can inadvertently mimic DoS attacks.\n",
    "- This may result in being banned from the website.\n",
    "\n",
    "**Preventive Measures:**\n",
    "- Insert random delays between requests to the server.\n",
    "- Example: Scrapy includes built-in safeguards like random delays between requests to avoid overloading servers. By default, Scrapy limits the risk of resembling a DoS attack.\n",
    "\n",
    "**Best Practices for Developers:**\n",
    "- Limit the number of pages scraped during development and debugging.\n",
    "- Use Scrapy’s allowed_domains property to restrict scraping to specific domains.\n",
    "- Avoid scraping at full scale until the scraper is thoroughly tested.\n",
    "\n",
    "**In summary:** With proper precautions, the risk of causing trouble is minimal! (Learn to play by the rules)\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Don’t steal: Copyright and fair use\n",
    "--- \n",
    "\n",
    "**Legal Considerations of Web Scraping:**\n",
    "- Scraping may be illegal if a website’s terms and conditions explicitly prohibit copying its content.\n",
    "- Violating such terms could lead to legal trouble.\n",
    "  \n",
    "**General Acceptance:**\n",
    "- Web scraping is often tolerated if it does not disrupt the regular use of a website.\n",
    "- It is comparable to using a browser to access publicly available web content.\n",
    "\n",
    "**Public vs. Protected Data:**\n",
    "- Scraping publicly available data (not behind authentication or paywalls) is generally acceptable.\n",
    "- Problems arise when scraped data is redistributed or shared inappropriately.\n",
    "\n",
    "**Copyright Concerns:**\n",
    "- Republishing scraped content, especially without permission, can constitute copyright infringement.\n",
    "- Simply posting content from one site onto another as one’s own is illegal.\n",
    "\n",
    "**Fair Use Exceptions:**\n",
    "- Using scraped data in aggregate or derivative formats may qualify as “fair use.”\n",
    "- Avoid passing off scraped data as original, copying it verbatim, or monetizing it without permission.\n",
    "\n",
    "**Legal Variability:**\n",
    "- Copyright and data privacy laws vary by country; check the laws applicable to your location.\n",
    "- Example: In Australia, scraping and storing personal information (e.g., names, phone numbers, email addresses) can be illegal, even if publicly available.\n",
    "\n",
    "**Personal vs. Large-Scale Use:**\n",
    "- For personal use, following general scraping guidelines is usually sufficient.\n",
    "- For large-scale research or commercial projects, seek legal advice beforehand.\n",
    "\n",
    "**University Resources:**\n",
    "- Universities often have a copyright office to assist with legal aspects of data scraping projects.\n",
    "- The university library is a good starting point for guidance on copyright issues.\n",
    "\n",
    "**Best Practices:** Scrape responsibly, respect terms of use, and ensure compliance with copyright laws.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## robots.txt\n",
    "---\n",
    "\n",
    "**What is robots.txt?**\n",
    "- A file placed at the root of a website (example.com/robots.txt) to communicate with web crawlers and bots.\n",
    "- It specifies which parts of the website can or cannot be accessed by automated programs.\n",
    "\n",
    "**Purpose of robots.txt:**\n",
    "- Helps manage server load by restricting unnecessary crawling.\n",
    "- Protects sensitive or non-public sections of a site from being indexed.\n",
    "- Provides guidelines for ethical web scraping and crawling.\n",
    "\n",
    "**How robots.txt Works:**\n",
    "- Uses directives to instruct bots (e.g., User-agent: * applies to all bots).\n",
    "\n",
    "**Common directives include:**\n",
    "- `Disallow`: specifies paths that bots should not crawl.\n",
    "- `Allow`: permits access to specific paths.\n",
    "- `Sitemap`: points bots to the website’s sitemap for better indexing.\n",
    "\n",
    "**Limitations of robots.txt:**\n",
    "- It is advisory, not enforceable; malicious or non-compliant bots can ignore it.\n",
    "- Does not provide security; sensitive content should be protected by other means (e.g., authentication).\n",
    "\n",
    "**Best Practices for Web Scraping:**\n",
    "- Always check a website’s robots.txt file before scraping.\n",
    "- Respect the rules specified in the file to avoid violating ethical or legal standards.\n",
    "- Use tools or libraries (e.g., Python’s robotsparser) to parse and adhere to robots.txt. https://docs.python.org/3/library/urllib.robotparser.html\n",
    "\n",
    "**Locating robots.txt:**\n",
    "- Visit the URL https://[website-domain]/robots.txt to view its contents.\n",
    "- Note that not all websites have a robots.txt file.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Web scraping code of conduct\n",
    "---\n",
    "\n",
    "If you adhere to the following simple rules, you will probably be fine.\n",
    "\n",
    "1. **Ask nicely.** If your project requires data from a particular organisation, for example, you can try asking them directly if they could provide you what you are looking for. With some luck, they will have the primary data that they used on their website in a structured format, saving you the trouble.\n",
    "2. **Don’t download copies of documents that are clearly not public.** For example, academic journal publishers often have very strict rules about what you can and what you cannot do with their databases. Mass downloading article PDFs is probably prohibited and can put you (or at the very least your friendly university librarian) in trouble. If your project requires local copies of documents (e.g. for text mining projects), special agreements can be reached with the publisher. The library is a good place to start investigating something like that.\n",
    "3. **Check your local legislation.** For example, certain countries have laws protecting personal information such as email addresses and phone numbers. Scraping such information, even from publicly avaialable web sites, can be illegal (e.g. in Australia).\n",
    "4. **Don’t share downloaded content illegally.** Scraping for personal purposes is usually OK, even if it is copyrighted information, as it could fall under the fair use provision of the intellectual property legislation. However, sharing data for which you don’t hold the right to share is illegal.\n",
    "Share what you can. If the data you scraped is in the public domain or you got permission to share it, then put it out there for other people to reuse it (e.g. on datahub.io). If you wrote a web scraper to access it, share its code (e.g. on GitHub) so that others can benefit from it.\n",
    "5. **Don’t break the Internet.** Not all web sites are designed to withstand thousands of requests per second. If you are writing a recursive scraper (i.e. that follows hyperlinks), test it on a smaller dataset first to make sure it does what it is supposed to do. Adjust the settings of your scraper to allow for a delay between requests. By default, Scrapy uses conservative settings that should minimize this risk.\n",
    "6. **Publish your own data in a reusable way.** Don’t force others to write their own scrapers to get at your data. Use open and software-agnostic formats (e.g. JSON, XML), provide metadata (data about your data: where it came from, what it represents, how to use it, etc.) and make sure it can be indexed by search engines so that people can find it.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Video recommendations \n",
    "---\n",
    "\n",
    "[David Kriesel - SpiegelMining – Reverse Engineering von Spiegel-Online @ 33c3](https://media.ccc.de/v/33c3-7912-spiegelmining_reverse_engineering_von_spiegel-online)\n",
    "\n",
    "<iframe width=\"512\" height=\"288\" src=\"https://media.ccc.de/v/33c3-7912-spiegelmining_reverse_engineering_von_spiegel-online/oembed\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\n",
    "[David Kriesel - BahnMining - Pünktlichkeit ist eine Zier @ 36C3](https://media.ccc.de/v/36c3-10652-bahnmining_-_punktlichkeit_ist_eine_zier)\n",
    "\n",
    "<iframe width=\"512\" height=\"288\" src=\"https://media.ccc.de/v/36c3-10652-bahnmining_-_punktlichkeit_ist_eine_zier/oembed\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "## Further reading\n",
    "---\n",
    "\n",
    "https://en.wikipedia.org/wiki/Data_journalism  \n",
    "https://en.wikipedia.org/wiki/Web_scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key points\n",
    "---\n",
    "\n",
    "- Web scraping is, in general, legal and won’t get you into trouble.\n",
    "\n",
    "- There are a few things to be careful about, notably don’t overwhelm a web server and don’t steal content.\n",
    "\n",
    "- Be nice. In doubt, ask.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab assignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping and evaluating NHL team stats since 1990\n",
    "\n",
    "Open this site to browse through a database of NHL team stats since 1990:\n",
    "\n",
    "https://www.scrapethissite.com/pages/forms/\n",
    "\n",
    "Luckily, the data is already provided in tabular format. However, not all of the data is shown in a single page. Instead, you will have to paginate through the database. In the following, your task is to scrape all of the data and find a way to automatically scrape all of the pages and fetch the complete database. Then write everything into a CSV file. The columns names should correspond to those in the HTML table. Afterwards you will load the CSV file with pandas and give answers to the following two questions:\n",
    "\n",
    "- How made the most \"wins\" in 1990, 2000, and 2010?\n",
    "- How many teams participated in 1991, 2001, and 2011?\n",
    "\n",
    "**In summary:**\n",
    "1. Scrape the data (find a way to obtain all data from a pages programmatically)\n",
    "2. Store the entire data as CSV file\n",
    "3. Load it with pandas\n",
    "4. Give answers to the questions above by making use of the DataFrame methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the example above, we recommend to use **requests** and **beautifulsoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['Team Name', 'Year', 'Wins', 'Losses', 'OT Losses', 'Win %',\n",
      "       'Goals For (GF)', 'Goals Against (GA)', '+ / -'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in the DataFrame:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Data successfully saved to nhl_team_stats.csv\n",
      "Columns in the DataFrame: Index(['Team Name', 'Year', 'Wins', 'Losses', 'OT Losses', 'Win %',\n",
      "       'Goals For (GF)', 'Goals Against (GA)', '+ / -'],\n",
      "      dtype='object')\n",
      "Most wins in 1990: Chicago Blackhawks with 49 wins\n",
      "Most wins in 2000: Colorado Avalanche with 52 wins\n",
      "Most wins in 2010: Vancouver Canucks with 54 wins\n",
      "Number of teams in 1991: 22\n",
      "Number of teams in 2001: 30\n",
      "Number of teams in 2011: 30\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the base URL and initialize data storage\n",
    "BASE_URL = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "data = []\n",
    "\n",
    "# Step 2: Scrape data from all pages\n",
    "def scrape_page(page_number):\n",
    "    \"\"\"Scrape a single page and return the extracted rows.\"\"\"\n",
    "    response = requests.get(BASE_URL, params={\"page_num\": page_number})\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page_number}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "    rows = table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "\n",
    "    page_data = []\n",
    "    for row in rows:\n",
    "        cols = [col.text.strip() for col in row.find_all(\"td\")]\n",
    "        if cols:\n",
    "            page_data.append(cols)\n",
    "    return page_data\n",
    "\n",
    "# Step 3: Automatically paginate and collect all data\n",
    "page_number = 1\n",
    "while True:\n",
    "    print(f\"Scraping page {page_number}...\")\n",
    "    page_data = scrape_page(page_number)\n",
    "    if not page_data:\n",
    "        break\n",
    "    data.extend(page_data)\n",
    "    page_number += 1\n",
    "\n",
    "# Step 4: Save the data to a CSV file\n",
    "# Dynamically determine the column names from the first row of the table\n",
    "response = requests.get(BASE_URL)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "header_row = table.find(\"tr\")\n",
    "columns = [header.text.strip() for header in header_row.find_all(\"th\")]\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "csv_filename = \"nhl_team_stats.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Data successfully saved to {csv_filename}\")\n",
    "\n",
    "# Step 5: Load the data with pandas and answer the questions\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Check the columns in the DataFrame\n",
    "print(\"Columns in the DataFrame:\", df.columns)\n",
    "\n",
    "# Convert numeric columns to integers for analysis\n",
    "df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "df[\"Wins\"] = df[\"Wins\"].astype(int)\n",
    "\n",
    "# Question 1: Most wins in 1990, 2000, 2010\n",
    "for year in [1990, 2000, 2010]:\n",
    "    most_wins_team = df[df[\"Year\"] == year].sort_values(\"Wins\", ascending=False).iloc[0]\n",
    "    print(f\"Most wins in {year}: {most_wins_team['Team Name']} with {most_wins_team['Wins']} wins\")\n",
    "\n",
    "# Question 2: Number of teams in 1991, 2001, 2011\n",
    "for year in [1991, 2001, 2011]:\n",
    "    num_teams = df[df[\"Year\"] == year][\"Team Name\"].nunique()\n",
    "    print(f\"Number of teams in {year}: {num_teams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have completed the tasks, please commit this notebook with the solution to your GitHub repository in the directory `assignments/08/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
